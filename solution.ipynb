{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving a Lunar Lander with differentiable Genetic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "To install the required libraries run the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test change\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Imports from the standard genepro-multi library are done here. Any adjustments (e.g. different operators) should be made in the notebook. For example:\n",
    "\n",
    "```\n",
    "class SmoothOperator(Node):\n",
    "  def __init__(self):\n",
    "    super(SmoothOperator,self).__init__()\n",
    "    self.arity = 1\n",
    "    self.symb = \"SmoothOperator\"\n",
    "\n",
    "  def _get_args_repr(self, args):\n",
    "    return self._get_typical_repr(args,'before')\n",
    "\n",
    "  def get_output(self, X):\n",
    "    c_outs = self._get_child_outputs(X)\n",
    "    return np.smoothOperation(c_outs[0])\n",
    "\n",
    "  def get_output_pt(self, X):\n",
    "    c_outs = self._get_child_outputs_pt(X)\n",
    "    return torch.smoothOperation(c_outs[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "import genepro\n",
    "importlib.reload(genepro)\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from genepro.node_impl import *\n",
    "from genepro.evo import Evolution\n",
    "from genepro.node_impl import Constant\n",
    "# importlib.reload(genepro.evo)\n",
    "# importlib.reload(genepro.node_impl)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#workaround for error 15:\n",
    "#OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Setup\n",
    "Here we first setup the Gymnasium environment. Please see https://gymnasium.farama.org/environments/box2d/lunar_lander/ for more information on the environment. \n",
    "\n",
    "Then a memory buffer is made. This is a buffer in which state transitions are stored. When the buffer reaches its maximum capacity old transitions are replaced by new ones.\n",
    "\n",
    "A frame buffer is initialised used to later store animation frames of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity): #specifies the max length of the memory buffer by making deque() object\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args): #pushes new transition(state,action,nextstate, reward) onto memory\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size): #returns sample batch of transitions in memory\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): #returns current length of the memory\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __iadd__(self, other): #changes and returns the existing memory\n",
    "      self.memory += other.memory\n",
    "      return self \n",
    "\n",
    "    def __add__(self, other): #leaves existing memory but creates and returns new combined memory\n",
    "      self.memory = self.memory + other.memory \n",
    "      return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Function\n",
    "\n",
    "Here you get to be creative. The default setup evaluates 5 episodes of 300 frames. Think of what action to pick and what fitness function to use. The Multi-tree takes an input of $n \\times d$ where $n$ is a batch of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function_pt(multitree, num_episodes=5, episode_duration=300, render=False, ignore_done=False):\n",
    "  memory = ReplayMemory(10000)\n",
    "  rewards = []\n",
    "\n",
    "  for _ in range(num_episodes):\n",
    "    # get initial state of the environment\n",
    "    observation = env.reset() #resets to initial state (first seed is chosen randomly, then stays the same)\n",
    "    observation = observation[0] #returns metric(?) of first observation\n",
    "    observation = np.array([(observation[0]/90), (observation[1]/90), (observation[2]/5), (observation[3]/5), (observation[4]/3.1415927), (observation[5]/5), observation[6], observation[7]])\n",
    "\n",
    "    for _ in range(episode_duration):\n",
    "      if render:\n",
    "        frames.append(env.render())\n",
    "\n",
    "      input_sample = torch.from_numpy(observation.reshape((1,-1))).float() #creates a tensor from a numpy array (shared memory)\n",
    "      \n",
    "      action = torch.argmax(multitree.get_output_pt(input_sample))\n",
    "      observation, reward, terminated, truncated, info = env.step(action.item()) #updates environment with input action\n",
    "      observation = np.array([(observation[0]/90), (observation[1]/90), (observation[2]/5), (observation[3]/5), (observation[4]/3.1415927), (observation[5]/5), observation[6], observation[7]])\n",
    "      \n",
    "      rewards.append(reward)\n",
    "      output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "      memory.push(input_sample, torch.tensor([[action.item()]]), output_sample, torch.tensor([reward]))\n",
    "      if (terminated or truncated) and not ignore_done:\n",
    "        break\n",
    "\n",
    "  fitness = np.sum(rewards)\n",
    "  \n",
    "  return fitness, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Setup\n",
    "Here the leaf and internal nodes are defined. Think about the odds of sampling a constant in this default configurations. Also think about any operators that could be useful and add them here. \n",
    "\n",
    "Adjust the population size (multiple of 8 if you want to use the standard tournament selection), max generations and max tree size to taste. Be aware that each of these settings can increase the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available: False\n",
      "multitreee traversal [['+', '0.0'], ['+', '0.0'], ['+', '0.0'], ['+', '0.0']]\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "tensor([[ 4.6081, -3.7208, -1.7102, -2.2054]], grad_fn=<CatBackward0>)\n",
      "multitreee traversal [['+', '0.0', '/', None], ['+', '0.0', '+', None], ['+', '0.0', 'min', None], ['+', '0.0', 'x_6']]\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: /\n",
      "converting operator: None\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: +\n",
      "converting operator: None\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: min\n",
      "converting operator: None\n",
      "converting operator: +\n",
      "converting operator: 0.0\n",
      "converting operator: x_6\n",
      "next states [tensor([[0.2897, 2.0000],\n",
      "        [0.6788, 0.0000],\n",
      "        [0.6954, 2.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], requires_grad=True), tensor([[0.2897, 2.0000],\n",
      "        [0.6788, 0.0000],\n",
      "        [0.2897, 2.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], requires_grad=True), tensor([[0.2897, 2.0000],\n",
      "        [0.6788, 0.0000],\n",
      "        [0.8098, 2.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], requires_grad=True), tensor([[0.2897, 2.0000],\n",
      "        [0.6788, 0.0000],\n",
      "        [0.8702, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], requires_grad=True)]\n",
      "rewards [1, 1, 1, 1]\n",
      "done True\n",
      "tree full [False, False, False, False]\n",
      "len replay mem 32\n",
      "len replay mem 32\n",
      "len replay mem 32\n",
      "len replay mem 32\n",
      "batch actions ['/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/', '/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\n",
      "Episode 0 Steps | CPU 13.3 | RAM 80.5:   0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\Sarah\\Documents\\Important\\Evolutionary Algorithms\\pycharm\\symbolic_dqn\\model.py:192: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(state_eval)\n",
      "C:\\Users\\Sarah\\anaconda3\\envs\\EA_env\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "\n",
      "Episode 0 Steps | CPU 13.3 | RAM 80.5:   0%|          | 0/500 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sarah\\Documents\\Important\\Evolutionary Algorithms\\pycharm\\symbolic_dqn\\main.py\", line 115, in <module>\n",
      "    losses = model.optimize_model(optimizers, policy_nets, target_nets, replay_memories, dqn_loss, BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA)       \n",
      "  File \"C:\\Users\\Sarah\\Documents\\Important\\Evolutionary Algorithms\\pycharm\\symbolic_dqn\\model.py\", line 232, in optimize_model\n",
      "    batch_actions = torch.tensor(np.array(batch_actions))\n",
      "TypeError: can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "!python symbolic_dqn\\main.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = env.observation_space.shape[0]\n",
    "leaf_nodes = [Feature(i) for i in range(num_features)]\n",
    "leaf_nodes = leaf_nodes + [Constant()] # Think about the probability of sampling a coefficient\n",
    "internal_nodes = [Plus(),Minus(),Times(),Div()] #Add your own operators here\n",
    "\n",
    "evo = Evolution(\n",
    "  fitness_function_pt, internal_nodes, leaf_nodes,\n",
    "  4,\n",
    "  pop_size=16,\n",
    "  max_gens=2,\n",
    "  max_tree_size=31,\n",
    "  n_jobs=4,\n",
    "  verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(tree):\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(5): #run 5 episodes for baseline\n",
    "      # get initial state\n",
    "      observation = env.reset(seed=i)\n",
    "      observation = observation[0]\n",
    "      observation = np.array([(observation[0]/90), (observation[1]/90), (observation[2]/5), (observation[3]/5), (observation[4]/3.1415927), (observation[5]/5), observation[6], observation[7]])\n",
    "\n",
    "      for _ in range(300): #300 time steps for baseline\n",
    "        # build up the input sample for GP\n",
    "        input_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        # get output (squeezing because it is encapsulated in an array)\n",
    "        output = tree.get_output_pt(input_sample)\n",
    "        action = torch.argmax(output) # What goes here?\n",
    "        observation, reward, terminated, truncated, info = env.step(action.item())\n",
    "        observation = np.array([(observation[0]/90), (observation[1]/90), (observation[2]/5), (observation[3]/5), (observation[4]/3.1415927), (observation[5]/5), observation[6], observation[7]])\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "\n",
    "        output_sample = torch.from_numpy(observation.reshape((1,-1))).float()\n",
    "        if (terminated or truncated):\n",
    "            break\n",
    "\n",
    "    fitness = np.sum(rewards)\n",
    "    \n",
    "    return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_optimisation():\n",
    "    for best in evo.population:\n",
    "        batch_size = 128\n",
    "        GAMMA = 0.99\n",
    "\n",
    "        constants = best.get_subtrees_consts()\n",
    "        if len(constants)>0:\n",
    "            optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "        for _ in range(500):\n",
    "\n",
    "            if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "                target_tree = copy.deepcopy(best)\n",
    "\n",
    "                transitions = evo.memory.sample(batch_size)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                \n",
    "                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "                non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.cat(batch.action)\n",
    "                reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "                state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "                next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "                with torch.no_grad():\n",
    "                    next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "                expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "                \n",
    "                criterion = nn.SmoothL1Loss()\n",
    "                loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "            \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "                optimizer.step()\n",
    "\n",
    "        best.fitness = evo.fitness_function(best)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolve\n",
    "Running this cell will use all the settings above as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mevo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      5\u001B[0m     coefficient_optimisation()\n",
      "File \u001B[1;32m~\\Documents\\Important\\Evolutionary Algorithms\\pycharm\\genepro\\evo.py:239\u001B[0m, in \u001B[0;36mEvolution.evolve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;66;03m# set the start time\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m--> 239\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_population\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;66;03m# generational loop\u001B[39;00m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_must_terminate():\n\u001B[0;32m    243\u001B[0m   \u001B[38;5;66;03m# perform one generation\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Important\\Evolutionary Algorithms\\pycharm\\genepro\\evo.py:168\u001B[0m, in \u001B[0;36mEvolution._initialize_population\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    165\u001B[0m   individual\u001B[38;5;241m.\u001B[39mget_readable_repr()\n\u001B[0;32m    167\u001B[0m \u001B[38;5;66;03m# evaluate the trees and store their fitness\u001B[39;00m\n\u001B[1;32m--> 168\u001B[0m fitnesses \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfitness_function\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopulation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m fitnesses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mfitnesses)))\n\u001B[0;32m    170\u001B[0m memories \u001B[38;5;241m=\u001B[39m fitnesses[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\EA_env\\lib\\site-packages\\joblib\\parallel.py:1098\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1095\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1098\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m elapsed_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_time\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\EA_env\\lib\\site-packages\\joblib\\parallel.py:975\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    974\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msupports_timeout\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m--> 975\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(\u001B[43mjob\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    976\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    977\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output\u001B[38;5;241m.\u001B[39mextend(job\u001B[38;5;241m.\u001B[39mget())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\EA_env\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[1;34m(future, timeout)\u001B[0m\n\u001B[0;32m    564\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[0;32m    565\u001B[0m \u001B[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 567\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CfTimeoutError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\EA_env\\lib\\concurrent\\futures\\_base.py:441\u001B[0m, in \u001B[0;36mFuture.result\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m--> 441\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_condition\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\EA_env\\lib\\threading.py:312\u001B[0m, in \u001B[0;36mCondition.wait\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[0;32m    311\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    313\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "evo.evolve()\n",
    "\n",
    "for i in range(1):\n",
    "    coefficient_optimisation()\n",
    "    evo.max_gens += 5\n",
    "    evo.evolve_continue()\n",
    "\n",
    "end_time = time.time()\n",
    "comp_time = end_time-start_time\n",
    "print(f\"Computation time: {comp_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_counter =0\n",
    "gen = range(len(evo.best_of_gens)) #generation\n",
    "avg_rewards = [] #average reward\n",
    "for i in evo.best_of_gens:\n",
    "    #print(\"for gen\",gen_counter)\n",
    "    avg_reward = get_test_score(i)/5\n",
    "    avg_rewards.append(avg_reward)\n",
    "    #print(\"average reward was:\",avg_reward)\n",
    "    #print(\"fitness was:\",i.fitness)\n",
    "    #gen_counter += 1\n",
    "#print(\"best of gens\",evo.best_of_gens)\n",
    "\n",
    "#plot generation vs fitness\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(gen,avg_rewards)\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average reward gained for five episodes by generation')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "best = evo.best_of_gens[-1]\n",
    "print(best.get_readable_repr())\n",
    "print(get_test_score(best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an animation\n",
    "Here the best evolved individual is selected and one episode is rendered. Make sure to save your lunar landers over time to track progress and make comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#test imagemagick\n",
    "#!magick --version\n",
    "\n",
    "#!conda install -c conda-forge imagemagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "# gist to save gif from https://gist.github.com/botforge/64cbb71780e6208172bbf03cd9293553\n",
    "def save_frames_as_gif(frames, path='./', filename='evolved_lander.gif'):\n",
    "  plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
    "  patch = plt.imshow(frames[0])\n",
    "  plt.axis('off')\n",
    "  def animate(i):\n",
    "      patch.set_data(frames[i])\n",
    "  anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "  anim.save(path + filename, writer='imagemagick', fps=60)\n",
    "\n",
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "The coefficients in the multi-tree aren't optimised. Here Q-learning (taken from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) is used to optimise the weights further. Incorporate coefficient optimisation in training your agent(s). Coefficient Optimisation can be expensive. Think about how often you want to optimise, when, which individuals etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "constants = best.get_subtrees_consts()\n",
    "print(\"before\", get_test_score(best))\n",
    "if len(constants)>0:\n",
    "  optimizer = optim.AdamW(constants, lr=1e-3, amsgrad=True)\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "  if len(constants)>0 and len(evo.memory)>batch_size:\n",
    "    target_tree = copy.deepcopy(best)\n",
    "\n",
    "    transitions = evo.memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                               if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = best.get_output_pt(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(batch_size, dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = target_tree.get_output_pt(non_final_next_states).max(1)[0].float()\n",
    "\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "   \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(constants, 100)\n",
    "    optimizer.step()\n",
    "\n",
    "print(best.get_readable_repr())\n",
    "print(\"after\", get_test_score(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "fitness_function_pt(best, num_episodes=1, episode_duration=500, render=True, ignore_done=False)\n",
    "env.close()\n",
    "save_frames_as_gif(frames, filename='evolved_lander_RL.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"evolved_lander_RL.gif\" width=\"750\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fd35777e",
   "language": "python",
   "display_name": "PyCharm (pycharm)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}